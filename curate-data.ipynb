{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45f21552",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset, Video\n",
    "from torch.utils.data import DataLoader, IterableDataset\n",
    "import webdataset as wds\n",
    "import os\n",
    "import base64\n",
    "import json\n",
    "import spacy\n",
    "from spacy import displacy\n",
    "import re\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7d06bff",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(os.cpu_count())\n",
    "num_threads = num_threads = min(32, (os.cpu_count() or 1) + 4)\n",
    "print(num_threads)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c82f9d0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "dataset = load_dataset(\"webdataset\", data_dir=\"/mmfs1/project/phan/tqn/RAG-VideoReferencing/data/\", streaming=True, split='train')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2bf06d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0844106f",
   "metadata": {},
   "outputs": [],
   "source": [
    "for idx, data in enumerate(dataset):\n",
    "    video_bytes = data[\"mp4\"]\n",
    "    temp_video_path = f\"temp_video_{idx}.mp4\"\n",
    "    with open(temp_video_path, \"wb\") as f:\n",
    "        f.write(video_bytes)\n",
    "    test = data['json']['captions']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8377d7e",
   "metadata": {},
   "source": [
    "# Process json file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6da7a2da",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: Clean texts with delete substring \n",
    "\n",
    "def clean_text(text):\n",
    "    text = text.strip().replace(\"\\n\", \" \")\n",
    "    # Remove filler words\n",
    "    text = re.sub(r'\\b(um|uh)\\b', '', text, flags=re.IGNORECASE)  # remove 'um' and 'uh' as full words\n",
    "    text = re.sub(r'\\b(okay|ok|o\\.k\\.?)\\b', '', text, flags=re.IGNORECASE)  # remove variations of 'okay'\n",
    "    text = re.sub(' +', ' ', text)  # remove extra spaces caused by removal\n",
    "    return text.strip()\n",
    "\n",
    "def is_substring(smaller, larger):\n",
    "    return smaller.lower() in larger.lower()\n",
    "\n",
    "def process_cleaning(captions, video_path):\n",
    "    print(f\"original num: {len(captions)} captions\")\n",
    "    caption_texts = []\n",
    "    for caption in captions:\n",
    "        caption_texts.append({\n",
    "            \"video_path\": video_path,\n",
    "            \"start\": caption[\"start\"], \n",
    "            \"end\": caption[\"end\"], \n",
    "            \"text\": clean_text(caption[\"text\"])\n",
    "        })\n",
    "    filtered_captions = []\n",
    "    for i, cap_i in enumerate(caption_texts):\n",
    "        is_sub = False\n",
    "        for j, cap_j in enumerate(caption_texts):\n",
    "            if i != j and is_substring(cap_i[\"text\"], cap_j[\"text\"]):\n",
    "                is_sub = True\n",
    "                break\n",
    "        if not is_sub:\n",
    "            filtered_captions.append(cap_i)\n",
    "    print(f\"STEP 2: Loaded {len(filtered_captions)} captions\")\n",
    "    return filtered_captions\n",
    "\n",
    "filtered_captions = []\n",
    "for idx, data in enumerate(dataset):\n",
    "    temp_video_path = f\"temp_video_{idx}.mp4\"\n",
    "    captions = data['json']['captions']\n",
    "    filtered_captions.append(process_cleaning(captions, temp_video_path))\n",
    "print(filtered_captions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c939243",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 2: Fix overlaps between consecutive captions\n",
    "def find_overlap(a, b):\n",
    "    max_overlap = min(len(a), len(b))\n",
    "    for i in range(max_overlap, 0, -1):\n",
    "        if a[-i:].lower() == b[:i].lower():\n",
    "            return i\n",
    "    return 0 \n",
    "\n",
    "def process_overlap_elimination(captions):\n",
    "    merged_chunks = []\n",
    "    for idx, cap in enumerate(captions):\n",
    "        if not merged_chunks:\n",
    "            merged_chunks.append(cap)\n",
    "        else:\n",
    "            prev = merged_chunks[-1]\n",
    "            overlap = find_overlap(prev['text'], cap['text'])\n",
    "            if overlap > 0:\n",
    "                cap_text_fixed = cap['text'][overlap:]\n",
    "            else:\n",
    "                cap_text_fixed = cap['text']\n",
    "            if cap_text_fixed:\n",
    "                merged_chunks.append({\n",
    "                    \"video_path\": cap[\"video_path\"],\n",
    "                    \"start\": cap[\"start\"],\n",
    "                    \"end\": cap[\"end\"],\n",
    "                    \"text\": cap_text_fixed\n",
    "                })\n",
    "    return merged_chunks\n",
    "total_captions = []\n",
    "for video in filtered_captions:\n",
    "    total_captions.append(process_overlap_elimination(video))\n",
    "print(total_captions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2b61a6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# step 3: Merge all text and split using spacy\n",
    "def clean_spaces(text):\n",
    "    return re.sub(r'\\s+', ' ', text).strip()\n",
    "\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "chunked_sentences = []\n",
    "for captions in total_captions:\n",
    "    full_text = \" \".join(chunk[\"text\"] for chunk in captions).strip()\n",
    "    doc = nlp(full_text)\n",
    "    \n",
    "    # Filter out sentences that are just \"okay\" (case insensitive)\n",
    "    sentences = []\n",
    "    for sent in doc.sents:\n",
    "        sent_text = clean_spaces(sent.text)\n",
    "        # Skip empty or very short sentences\n",
    "        if not sent_text.strip() or len(sent_text) <= 1:\n",
    "            continue\n",
    "            \n",
    "        # Count words in the sentence (splitting by whitespace)\n",
    "        words = sent_text.strip().split()\n",
    "        if len(words) <= 1:\n",
    "            print(f\"Filtering out single-word sentence: '{sent_text}'\")\n",
    "            continue\n",
    "        sentences.append(sent_text)\n",
    "    \n",
    "    # Only add sentences if we found more than one valid sentence\n",
    "    if len(sentences) > 1:\n",
    "        chunked_sentences.append(sentences)\n",
    "    \n",
    "    print(f\"STEP 3: Found {len(sentences)} valid sentences after filtering\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74aaf2d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "for sentence in chunked_sentences[6]:\n",
    "    print(sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60bd5c6c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aedfe657",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(chunked_sentences))\n",
    "print(len(total_captions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d380989",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 6: Find new timestamps for each segmented sentence\n",
    "chunked_results = []\n",
    "results = []\n",
    "chunk_pointer = 0\n",
    "for idx, sentences in enumerate(chunked_sentences):\n",
    "    results = []\n",
    "    chunk_pointer = 0\n",
    "    for sentence in sentences:\n",
    "        print(sentence)\n",
    "        sentence_length = len(sentence)\n",
    "        accumulated_text = \"\"\n",
    "        sentence_start_time = None\n",
    "        sentence_end_time = None\n",
    "\n",
    "        while chunk_pointer < len(total_captions[idx]) and len(accumulated_text) < sentence_length:\n",
    "            chunk = total_captions[idx][chunk_pointer]\n",
    "            if sentence_start_time is None:\n",
    "                sentence_start_time = chunk[\"start\"]\n",
    "            if accumulated_text == \"\":\n",
    "                accumulated_text = chunk[\"text\"].strip()\n",
    "            else:\n",
    "                # Check if we need to add a space between chunks\n",
    "                if accumulated_text[-1] != \" \" and chunk[\"text\"] and chunk[\"text\"][0] != \" \":\n",
    "                    accumulated_text += \" \" + chunk[\"text\"]\n",
    "                else:\n",
    "                    accumulated_text += chunk[\"text\"]\n",
    "            sentence_end_time = chunk[\"end\"]\n",
    "            chunk_pointer += 1\n",
    "        if not sentence_start_time or not sentence_end_time: continue \n",
    "        if len(sentence) < 100: continue\n",
    "        results.append({\n",
    "            \"video_path\": f\"temp_video_{idx}.mp4\",\n",
    "            \"start\": sentence_start_time,\n",
    "            \"end\": sentence_end_time,\n",
    "            \"sentence\": sentence\n",
    "        })\n",
    "    chunked_results.append(results)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7119b083",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(chunked_results))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "372616c4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c869c68",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import cv2\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "from bertopic.backend import MultiModalBackend\n",
    "import umap\n",
    "import hdbscan\n",
    "import faiss\n",
    "from sklearn.preprocessing import normalize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3a7e5d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#steps 7: Extract frames\n",
    "import cv2\n",
    "import clip\n",
    "import torch\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "import umap\n",
    "import os\n",
    "import json\n",
    "import glob \n",
    "from sklearn.preprocessing import normalize\n",
    "from qdrant_client import QdrantClient\n",
    "from qdrant_client.http.models import PointStruct, Distance, VectorParams, Filter, FieldCondition, MatchValue\n",
    "import uuid\n",
    "\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "model, preprocess = clip.load(\"ViT-B/32\", device=device)\n",
    "\n",
    "def extract_frames_between_times(video_path, start_time, end_time, frame_rate=1):\n",
    "    cap = cv2.VideoCapture(video_path)\n",
    "    fps = cap.get(cv2.CAP_PROP_FPS)\n",
    "    \n",
    "    start_sec = time_to_seconds(start_time)\n",
    "    end_sec = time_to_seconds(end_time)\n",
    "    \n",
    "    start_frame = int(start_sec * fps)\n",
    "    end_frame = int(end_sec * fps)\n",
    "    \n",
    "    frames = []\n",
    "    cap.set(cv2.CAP_PROP_POS_FRAMES, start_frame)\n",
    "    for frame_num in range(start_frame, end_frame, int(fps / frame_rate)):\n",
    "        cap.set(cv2.CAP_PROP_POS_FRAMES, frame_num)\n",
    "        ret, frame = cap.read()\n",
    "        if not ret:\n",
    "            break\n",
    "        frames.append(frame)\n",
    "    cap.release()\n",
    "    return frames\n",
    "\n",
    "def time_to_seconds(time_str):\n",
    "    h, m, s = time_str.split(':')\n",
    "    return int(h) * 3600 + int(m) * 60 + float(s)\n",
    "\n",
    "@torch.no_grad()\n",
    "def get_multimodal_embeddings(docs, image_tensor, model_name='clip-ViT-B-32', batch_size=32):\n",
    "\n",
    "    # Truncate text if too long (CLIP has a 77 token limit)\n",
    "    truncated_text = ' '.join(docs.split()[:50])   # Simple truncation strategy\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        # Tokenize with truncation enabled\n",
    "        text_tokens = clip.tokenize([truncated_text], truncate=True).to(device)\n",
    "        \n",
    "        # Process text\n",
    "        text_features = model.encode_text(text_tokens)\n",
    "        \n",
    "        # Process images\n",
    "        if not isinstance(image_tensor, list):\n",
    "            image_tensor = [image_tensor]  # Convert single image to list\n",
    "            \n",
    "        # Preprocess all images\n",
    "        processed_images = []\n",
    "        for img in image_tensor:\n",
    "            processed_images.append(preprocess(img))\n",
    "        \n",
    "        # Stack images into a batch tensor\n",
    "        image_tensor = torch.stack(processed_images).to(device)\n",
    "        \n",
    "        # Process in batches if needed\n",
    "        if len(processed_images) > batch_size:\n",
    "            image_features_list = []\n",
    "            for i in range(0, len(processed_images), batch_size):\n",
    "                batch = image_tensor[i:i+batch_size]\n",
    "                batch_features = model.encode_image(batch)\n",
    "                image_features_list.append(batch_features)\n",
    "            image_features = torch.cat(image_features_list, dim=0)\n",
    "        else:\n",
    "            image_features = model.encode_image(image_tensor)\n",
    "        \n",
    "        # Average image features if multiple images\n",
    "        if len(processed_images) > 1:\n",
    "            image_features = torch.mean(image_features, dim=0, keepdim=True)\n",
    "        \n",
    "        # Normalize features\n",
    "        text_features = text_features / text_features.norm(dim=1, keepdim=True)\n",
    "        image_features = image_features / image_features.norm(dim=1, keepdim=True)\n",
    "        # Combine features (average pooling)\n",
    "        fused = (text_features + image_features) / 2.0\n",
    "        \n",
    "        return fused.cpu().numpy().squeeze()\n",
    "\n",
    "def reduce_dimensions(embeddings, n_components=20):\n",
    "    reducer = umap.UMAP(n_components=n_components, metric='cosine')\n",
    "    reduced = reducer.fit_transform(embeddings)\n",
    "    return reduced\n",
    "\n",
    "# def cluster_embeddings(embeddings, min_cluster_size=5):\n",
    "#     clusterer = hdbscan.HDBSCAN(min_cluster_size=min_cluster_size, metric='euclidean')\n",
    "#     labels = clusterer.fit_predict(embeddings)\n",
    "#     return labels\n",
    "\n",
    "def search(index, query_embedding, top_k=5):\n",
    "    normalized_query = normalize(query_embedding.reshape(1, -1), norm='l2')\n",
    "    distances, indices = index.search(normalized_query, top_k)\n",
    "    return distances, indices\n",
    "\n",
    "all_embeddings = []\n",
    "points = []\n",
    "for idx, results in enumerate(chunked_results):\n",
    "    temp_video_path = f\"temp_video_{idx}.mp4\"\n",
    "    for result in results:\n",
    "        frames = extract_frames_between_times(temp_video_path, result['start'], result['end'], frame_rate=1)\n",
    "        images = [Image.fromarray(cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)) for frame in frames]\n",
    "        embeddings = get_multimodal_embeddings(result['sentence'], image_tensor=images)\n",
    "        all_embeddings.append(embeddings)\n",
    "        payload = {\n",
    "            \"video_path\": result[\"video_path\"],\n",
    "            \"start\": result[\"start\"], \n",
    "            \"end\": result[\"end\"], \n",
    "            \"text\": result[\"sentence\"],\n",
    "        }\n",
    "        points.append(PointStruct(id=str(uuid.uuid4()), vector=embeddings.tolist(), payload=payload))\n",
    "\n",
    "reduced_embeddings = reduce_dimensions(all_embeddings, n_components = 2)\n",
    "print(reduced_embeddings.shape)\n",
    "\n",
    "# we will use all embedding and reduced embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59deec76",
   "metadata": {},
   "outputs": [],
   "source": [
    "client = QdrantClient(\n",
    "    url=\"https://265484ec-5f64-40ec-a619-c7c9dffc2dd9.us-east-1-0.aws.cloud.qdrant.io:6333\", \n",
    "    api_key=\"eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJhY2Nlc3MiOiJtIn0.I2MgcVnOKkWmOXwFlqJqEqm6LFQIF4cjxU5up4wxwyw\",\n",
    ")\n",
    "\n",
    "print(client.get_collections())\n",
    "\n",
    "COLLECTION_NAME = \"video_segments\"\n",
    "\n",
    "# Step 1: Create Qdrant Collection (if not exists)\n",
    "client.recreate_collection(\n",
    "    collection_name=COLLECTION_NAME,\n",
    "    vectors_config=VectorParams(size=512, distance=Distance.COSINE),\n",
    ")\n",
    "client.upsert(collection_name=COLLECTION_NAME, points=points)\n",
    "\n",
    "print(\"Uploaded to Qdrant.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c216a0d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "info = client.get_collection(COLLECTION_NAME)\n",
    "print(f\"Total points: {info.points_count}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bcc5b4a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 5: Query with TEXT ONLY\n",
    "def search_with_text_query(text_query: str, top_k=5):\n",
    "    with torch.no_grad():\n",
    "        text_tokens = clip.tokenize([text_query]).to(device)\n",
    "        text_features = model.encode_text(text_tokens)\n",
    "        text_features /= text_features.norm(dim=1, keepdim=True)\n",
    "\n",
    "    # Search in Qdrant\n",
    "    search_result = client.search(\n",
    "        collection_name=COLLECTION_NAME,\n",
    "        query_vector=text_features.cpu().numpy()[0].tolist(),\n",
    "        limit=top_k,\n",
    "    )\n",
    "\n",
    "    for i, hit in enumerate(search_result):\n",
    "        print(f\"\\nRank {i+1}:\")\n",
    "        print(f\"Score: {hit.score}\")\n",
    "        print(f\"Time: {hit.payload['start']} - {hit.payload['end']}\")\n",
    "        print(f\"Subtitle: {hit.payload['text']}\")\n",
    "        print(f\"video path: {hit.payload['video_path']}\")\n",
    "\n",
    "# üîç Example query\n",
    "search_with_text_query(\"\"\"What is regression problem for image processing\"\"\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
